{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2wC_HzxUHsa"
   },
   "source": [
    "# What is Ensemble?\n",
    "\n",
    "\n",
    "In statistics and machine learning, **ensemble methods** use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. [Wikipedia](https://en.wikipedia.org/wiki/Ensemble_learning)\n",
    "\n",
    "In other words, Ensemble methods help improve machine learning results by combining several models. This approach allows the production of better predictive performance compared to a single weak learner. \n",
    "\n",
    "![Ensemble](https://miro.medium.com/max/1745/1*u3acTccfZ2HYWilggOmFCA.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of the model is extremely important to have any chance to obtain good results.\n",
    "This choice can depend on many variables of the problem: quantity of data, dimensionality of the space, etc..\n",
    "\n",
    "When we try to predict the target variable by any machine learning algorithm, the main causes of the difference between the actual and predicted values are variance and bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kkcA-urpVQDL"
   },
   "source": [
    "![ensemble_principle](https://www.forecast.app/hs-fs/hubfs/accuracy-precision.jpg?width=454&name=accuracy-precision.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics and machine learning, the bias–variance tradeoff is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa. The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set.[wiki](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)\n",
    "\n",
    "![bias-variance trade off](https://miro.medium.com/max/1733/1*kISLC1Udq0m6g5kwHhMuJg@2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of ensemble methods is to try reducing bias and/or variance of such weak learners by combining several of them together in order to create a strong learner (or ensemble model) that achieves better performances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use Ensemble?\n",
    "---\n",
    "\n",
    "1. You can avoid bad luck.\n",
    "\n",
    "2. Performance can be improved.\n",
    "\n",
    "3. Overcome difficulties depending on the amount of data.\n",
    "\n",
    "4. Overcome difficulties according to data quality.\n",
    "\n",
    "5. Effective for multiple sensor systems.\n",
    "\n",
    "6. Incremental learning is possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PyiVzM7QXqFv"
   },
   "source": [
    "# Ensemble Types\n",
    "\n",
    "We will only discuss the three major meta-algorithms.\n",
    "\n",
    "\n",
    "Ensemble methods are meta-algorithms that combine several machine learning algorithms into one predictive model in order to decrease variance (bagging), reduce the bias (boosting), or improve predictions (stacking).\n",
    "\n",
    "![Ensemble_Type](https://miro.medium.com/max/2237/1*P0ns6A56MtpGFMQ2g47IYA.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **bagging**, that often considers homogeneous weak learners, learns them independently from each other in parallel and combines them following some kind of deterministic averaging process\n",
    "- **boosting**, that often considers homogeneous weak learners, learns them sequentially in a very adaptative way (a base model depends on the previous ones) and combines them following a deterministic strategy\n",
    "- **stacking**, that often considers heterogeneous weak learners, learns them in parallel and combines them by training a meta-model to output a prediction based on the different weak models predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. boostrapping process\n",
    "\n",
    "In statistics, bootstrapping is any test or metric that relies on random sampling with replacement. [Wikipedia](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))\n",
    "\n",
    "\n",
    "> The bootstrap was published by Bradley Efron\n",
    "\n",
    "![Illustration of the bootstrapping process.](https://miro.medium.com/max/1260/1*lWnm3eJVe3uo95OcSg5jUA@2x.png)\n",
    "\n",
    "\n",
    "![bootstarap](https://miro.medium.com/max/1815/1*7XVde-bMixpKf8mj61qhJQ@2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. bagging\n",
    "Bagging stands for **b**ootstrap **agg**regation. One way to reduce the variance of an estimate is to average together multiple estimates. For example, we can train M different models on different subsets of the data (chosen randomly with replacement) and compute the ensemble:\n",
    "\n",
    "$$ f(x) = 1/M \\sum_{m=1}^{M}f_m(x) $$\n",
    "\n",
    "Bagging uses bootstrap sampling to obtain the data subsets for training the base learners. For aggregating the outputs of base learners, bagging uses *voting for classification* and *averaging for regression*.\n",
    "\n",
    "> Combining stable learners is less advantageous since the ensemble will not help improve generalization performance.\n",
    "\n",
    "One of the big advantages of bagging is that it can be parallelised.\n",
    "\n",
    "![bagging](https://miro.medium.com/max/1815/1*zAMhmZ78a6V9W878zfk5eA@2x.png)\n",
    "\n",
    "![bagging algorithm](http://jcsites.juniata.edu/faculty/rhodes/ml/images/baggingalg.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : random forest\n",
    "\n",
    "A commonly used class of ensemble algorithms are [**forests of randomized trees**](https://scikit-learn.org/stable/modules/ensemble.html#forest).\n",
    "\n",
    "In random forests, each tree in the ensemble is built from a sample drawn with replacement (i.e. a bootstrap sample) from the training set. In addition, instead of using all the features, a random subset of features is selected, further randomizing the tree.\n",
    "\n",
    "As a result, the bias of the forest increases slightly, but due to the averaging of less correlated trees, its variance decreases, resulting in an overall better model.\n",
    "\n",
    "![RandomForest](https://miro.medium.com/max/988/0*uGzCQfXlC-97VR10.)\n",
    "\n",
    "In an [**extremely randomized trees**](https://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees) algorithm randomness goes one step further: the splitting thresholds are randomized. Instead of looking for the most discriminative threshold, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows reduction of the variance of the model a bit more, at the expense of a slightly greater increase in bias.\n",
    "![randomforest](https://miro.medium.com/max/1815/1*_B5HX2whbTs3DS8M6YBD_w@2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Boosting\n",
    "\n",
    "Boosting refers to a family of algorithms that are able to convert weak learners to strong learners. The main principle of boosting is to fit a sequence of weak learners− models that are only slightly better than random guessing. More weight is given to examples that were misclassified.\n",
    "\n",
    "The predictions are then combined through a weighted majority vote (classification) or a weighted sum (regression) to produce the final prediction. The principal difference between boosting and the committee methods, such as bagging, is that base learners are trained in sequence on a weighted version of the data.\n",
    "\n",
    "![boosting](https://miro.medium.com/max/1815/1*VGSoqefx3Rz5Pws6qpLwOQ@2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : AdaBoost\n",
    "\n",
    "The algorithm below describes the most widely used form of boosting algorithm called [**AdaBoost**](https://scikit-learn.org/stable/modules/ensemble.html#adaboost), which stands for adaptive boosting.\n",
    "\n",
    "![adaboost_algorithm](https://miro.medium.com/max/1318/0*MmYd6wgreP-oBoKi.)\n",
    "\n",
    "We see that the first base classifier $y1(x)$ is trained using weighting coefficients that are all equal. In subsequent boosting rounds, the weighting coefficients are increased for data points that are misclassified and decreased for data points that are correctly classified.\n",
    "\n",
    "The quantity epsilon represents a weighted error rate of each of the base classifiers. Therefore, the weighting coefficients alpha give greater weight to the more accurate classifiers.\n",
    "\n",
    "![Adaboost](https://miro.medium.com/max/1815/1*6JbndZ2zY2c4QqS73HQ47g@2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Gradient Boosting\n",
    "\n",
    "[**Gradient Tree Boosting**](https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting) is a generalization of boosting to arbitrary differentiable loss functions. It can be used for both regression and classification problems. Gradient Boosting builds the model in a sequential way.\n",
    "\n",
    "$$ F_m(x) = F_{m-1}(x) + \\gamma_m h_m (x)$$\n",
    "\n",
    "At each stage the descison tree $h_m(x)$ is chosen to minimize a loss function $L$ given the current model $F_{m-1}(x)$:\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + argmin_h \\sum_{i=1}^{n} L(y_{i}, F_{m-1}(x_{i}) + h(x_{i}))$$\n",
    "\n",
    "> The algorithms for regression and classification differ in the type of loss function used.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/ensemble.html#mathematical-formulation\n",
    "\n",
    "[Gradient boosting wiki](https://en.wikipedia.org/wiki/Gradient_boosting)\n",
    "\n",
    "![gradient boosting](https://miro.medium.com/max/1815/1*DK2iShmkQKibMz-mNcMwng@2x.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stacking\n",
    "\n",
    "Stacking is an ensemble learning technique that combines multiple classification or regression models via a meta-classifier or a meta-regressor. The base level models are trained based on a complete training set, then the meta-model is trained on the outputs of the base level model as features.\n",
    "\n",
    "The base level often consists of different learning algorithms and therefore stacking ensembles are often heterogeneous. The algorithm below summarizes stacking.\n",
    "\n",
    "![stacking_algorithm](https://miro.medium.com/max/1405/0*GXMZ7SIXHyVzGCE_.)\n",
    "\n",
    "Stacking is a commonly used technique for winning the Kaggle data science competition. For example, the first place for the Otto Group Product Classification challenge was won by a stacking ensemble of over 30 models whose output was used as features for three meta-classifiers: XGBoost, Neural Network, and Adaboost.\n",
    "\n",
    "\n",
    "![staking](https://miro.medium.com/max/1261/1*ZucZsXkOwrpY2XaPh6teRw@2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UQOGQCBNerE"
   },
   "source": [
    "### Example : Sklearn Stacking method\n",
    "\n",
    "The idea behind the [**VotingClassifier**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMwaUkBa5MU7"
   },
   "source": [
    "**Reference**\n",
    "\n",
    "[Ensemble Learning- The heart of Machine learning](https://medium.com/ml-research-lab/ensemble-learning-the-heart-of-machine-learning-b4f59a5f9777)\n",
    "\n",
    "[Ensemble Learning to Improve Machine Learning Results](https://blog.statsbot.co/ensemble-learning-d1dcd548e936)\n",
    "\n",
    "[Sklearn Ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html)\n",
    "\n",
    "[Ensemble methods_ bagging, boosting and stacking - Towards Data Science](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EnsembleLectureMaterial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
